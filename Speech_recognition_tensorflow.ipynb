{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Speech_recognition_tensorflow",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8NRSJlmEccH",
        "colab_type": "code",
        "outputId": "61d21972-a7af-4957-8130-7d0740e75642",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuFa8v2P6ock",
        "colab_type": "code",
        "outputId": "c50b527b-fedc-416d-db69-0ad0ee67a39c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "pip install python_speech_features"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting python_speech_features\n",
            "  Downloading https://files.pythonhosted.org/packages/ff/d1/94c59e20a2631985fbd2124c45177abaa9e0a4eee8ba8a305aa26fc02a8e/python_speech_features-0.6.tar.gz\n",
            "Building wheels for collected packages: python-speech-features\n",
            "  Building wheel for python-speech-features (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-speech-features: filename=python_speech_features-0.6-cp27-none-any.whl size=5888 sha256=a9ea3848c9ff27a786372fe88c1e7c462701e9fcbe0851545bbd82ad683e7c41\n",
            "  Stored in directory: /root/.cache/pip/wheels/3c/42/7c/f60e9d1b40015cd69b213ad90f7c18a9264cd745b9888134be\n",
            "Successfully built python-speech-features\n",
            "Installing collected packages: python-speech-features\n",
            "Successfully installed python-speech-features-0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuNVxfHH8vfa",
        "colab_type": "code",
        "outputId": "d95fb2f8-dd3c-43c1-de1e-d1dbecaf219d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2nW1V0D9SFC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import librosa\n",
        "import pickle\n",
        "import numpy as np\n",
        "from python_speech_features import mfcc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHq6zNhI9Wdl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing traning mfccs and labels\n",
        "with open('/content/gdrive/My Drive/Colab Notebooks/mfcc_train.pkl', 'rb') as m:\n",
        "    mfcc_train = pickle.load(m)\n",
        "with open('/content/gdrive/My Drive/Colab Notebooks/train_labels.pkl', 'rb') as n:\n",
        "    train_labels = pickle.load(n)\n",
        "\n",
        "#importing valid mfccs and labels\n",
        "with open('/content/gdrive/My Drive/Colab Notebooks/mfcc_valid.pkl', 'rb') as o:\n",
        "    mfcc_valid = pickle.load(o)\n",
        "with open('/content/gdrive/My Drive/Colab Notebooks/valid_labels.pkl', 'rb') as p:\n",
        "    valid_labels = pickle.load(p)\n",
        "    \n",
        "#importing test mfccs and labels\n",
        "with open('/content/gdrive/My Drive/Colab Notebooks/mfcc_test.pkl', 'rb') as q:\n",
        "    mfcc_test = pickle.load(q)\n",
        "with open('/content/gdrive/My Drive/Colab Notebooks/test_labels.pkl', 'rb') as r:\n",
        "    test_labels = pickle.load(r)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKEHa4C69cjn",
        "colab_type": "code",
        "outputId": "de4be5d4-1ff4-4e7d-c756-982d4735a94b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Labels\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y_train = le.fit_transform(train_labels)\n",
        "y_valid = le.fit_transform(valid_labels)\n",
        "y_test = le.fit_transform(test_labels)\n",
        "classes = list(le.classes_)\n",
        "len(classes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJ1gHhEw9hYL",
        "colab_type": "code",
        "outputId": "3d26c1c2-45a9-4200-9d6a-c5066cbae512",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#converting labels to one hot encoding\n",
        "from keras.utils import np_utils\n",
        "y_train=np_utils.to_categorical(y_train, num_classes=len(classes))\n",
        "y_valid=np_utils.to_categorical(y_valid, num_classes=len(classes))\n",
        "y_test=np_utils.to_categorical(y_test, num_classes=len(classes))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lrql0NEQ9kd4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Check the shape of mfcc array\n",
        "mfcc_train = np.array(mfcc_train)\n",
        "mfcc_valid = np.array(mfcc_valid)\n",
        "mfcc_test = np.array(mfcc_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_3FNwNV9ppc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "# Feature Scaling on training dataset\n",
        "num_instances, num_time_steps, num_features = mfcc_train.shape\n",
        "mfcc_train = np.reshape(mfcc_train, newshape=(-1, num_features))\n",
        "mfcc_train = sc.fit_transform(mfcc_train)\n",
        "mfcc_train = np.reshape(mfcc_train, newshape=(num_instances, num_time_steps, num_features))\n",
        "# Feature Scaling on validation dataset\n",
        "num_instances, num_time_steps, num_features = mfcc_valid.shape\n",
        "mfcc_valid = np.reshape(mfcc_valid, newshape=(-1, num_features))\n",
        "mfcc_valid = sc.transform(mfcc_valid)\n",
        "mfcc_valid = np.reshape(mfcc_valid, newshape=(num_instances, num_time_steps, num_features))\n",
        "# Feature Scaling on testing dataset\n",
        "num_instances, num_time_steps, num_features = mfcc_test.shape\n",
        "mfcc_test = np.reshape(mfcc_test, newshape=(-1, num_features))\n",
        "mfcc_test = sc.fit_transform(mfcc_test)\n",
        "mfcc_test = np.reshape(mfcc_test, newshape=(num_instances, num_time_steps, num_features))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHj4T5x1XFAy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#RNN Model\n",
        "from tensorflow.contrib import rnn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X256KAJx9qyh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_input = mfcc_train.shape[2] #MFCC features\n",
        "n_steps = mfcc_train.shape[1] #no. of timesteps\n",
        "n_hidden = 64\n",
        "n_classes = len(classes)\n",
        "batch_size = 100\n",
        "num_epochs = 100\n",
        "no_of_batches = 100\n",
        "#num_of_layers = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ml7TEl5N9vpF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tf Graph input\n",
        "x = tf.placeholder(tf.float32, [None, n_steps, n_input])\n",
        "y = tf.placeholder(tf.float32, [None, n_classes])\n",
        "weights = {\n",
        "   'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
        "}\n",
        "biases = {\n",
        "   'out': tf.Variable(tf.random_normal([n_classes]))\n",
        "}\n",
        "keep_prob = tf.placeholder(tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3KcAwkB9zH9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#batch\n",
        "def next_batch(num, data, labels):\n",
        "    '''\n",
        "    Return a total of `num` random samples and labels. \n",
        "    '''\n",
        "    idx = np.arange(0 , len(data))\n",
        "    np.random.shuffle(idx)\n",
        "    idx = idx[:num]\n",
        "    data_shuffle = [data[ i] for i in idx]\n",
        "    labels_shuffle = [labels[ i] for i in idx]\n",
        "\n",
        "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jquCoP0s93qj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#RNN declaration\n",
        "def RNN(x, weights, biases):\n",
        "    x = tf.unstack(x, n_steps, 1)\n",
        "\n",
        "   # Define a lstm cell with tensorflow\n",
        "    lstm_cell = rnn.BasicLSTMCell(n_hidden, forget_bias = 0.2)\n",
        "    \n",
        "   #Dropout\n",
        "   #lstm_dropout = rnn.DropoutWrapper(lstm_cell, output_keep_prob = 0.2)\n",
        "   \n",
        "   # Get lstm cell output\n",
        "    outputs, states = rnn.static_rnn(lstm_cell, x, dtype = tf.float32)\n",
        "\n",
        "   # Linear activation, using rnn inner loop last output\n",
        "    return tf.matmul(outputs[-1], weights['out']) + biases['out']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSqfhdSr97Lg",
        "colab_type": "code",
        "outputId": "035dc796-9fa7-44f5-eec9-759440aadfd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "#prediction\n",
        "pred = RNN(x, weights, biases)\n",
        "\n",
        "# Define loss and optimizer\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = pred, labels = y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate = 0.001).minimize(cost)\n",
        "\n",
        "# Evaluate model\n",
        "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# Initializing the variables\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0906 06:41:20.257649 140360891025280 deprecation.py:323] From <ipython-input-15-959b7414dfd7>:5: __init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "W0906 06:41:20.259623 140360891025280 deprecation.py:323] From <ipython-input-15-959b7414dfd7>:11: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
            "W0906 06:41:20.288933 140360891025280 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0906 06:41:20.301045 140360891025280 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:738: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0906 06:41:22.112219 140360891025280 deprecation.py:323] From <ipython-input-16-c042296c4bd2>:4: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLfFVrqh-H9d",
        "colab_type": "code",
        "outputId": "d1a56358-8488-4279-fc1b-c1be4505f3e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        loss_train = 0;\n",
        "        loss_valid = 0;\n",
        "        for i in range(no_of_batches):  \n",
        "            batch_x, batch_y = next_batch(batch_size, mfcc_train, y_train)\n",
        "            batch_x_val, batch_y_val = next_batch(batch_size, mfcc_valid, y_valid)\n",
        "            batch_x = batch_x.reshape((batch_size, n_steps, n_input))\n",
        "            batch_x_val = batch_x_val.reshape((batch_size, n_steps, n_input))\n",
        "            sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
        "            training_acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y})\n",
        "            valid_acc = sess.run(accuracy, feed_dict={x: batch_x_val, y: batch_y_val})\n",
        "            loss_train += sess.run(cost, feed_dict={x: batch_x, y: batch_y})\n",
        "            loss_valid += sess.run(cost, feed_dict={x: batch_x_val, y: batch_y_val})\n",
        "        avg_loss_training = loss_train / no_of_batches\n",
        "        avg_loss_validation = loss_valid / no_of_batches\n",
        "        print(\"Epoch \" + str(epoch) + \", Training Loss= \" + \\\n",
        "              \"{:.6f}\".format(avg_loss_training) + \", Training Accuracy= \" + \\\n",
        "              \"{:.5f}\".format(training_acc) + \", Validation Loss= \" + \\\n",
        "              \"{:.5f}\".format(avg_loss_validation) + \", Validation Accuracy= \" + \\\n",
        "              \"{:.5f}\".format(valid_acc))\n",
        "    saver.save(sess, 'content/my_test_model')   \n",
        "    print(\"Optimization Finished!\")\n",
        "    validate_feed = {x: mfcc_valid, y: y_valid}\n",
        "    print(\"Validation Accuracy:\", \\\n",
        "        sess.run(accuracy, feed_dict=validate_feed))\n",
        "    test_feed = {x:mfcc_test , y: y_test}\n",
        "    print(\"Testing Accuracy:\", \\\n",
        "        sess.run(accuracy, feed_dict=test_feed))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, Training Loss= 1.125884, Training Accuracy= 0.83000, Validation Loss= 1.12737, Validation Accuracy= 0.83000\n",
            "Epoch 1, Training Loss= 0.815815, Training Accuracy= 0.80000, Validation Loss= 0.77235, Validation Accuracy= 0.77000\n",
            "Epoch 2, Training Loss= 0.764174, Training Accuracy= 0.82000, Validation Loss= 0.77281, Validation Accuracy= 0.77000\n",
            "Epoch 3, Training Loss= 0.775539, Training Accuracy= 0.84000, Validation Loss= 0.77249, Validation Accuracy= 0.78000\n",
            "Epoch 4, Training Loss= 0.773679, Training Accuracy= 0.78000, Validation Loss= 0.72272, Validation Accuracy= 0.84000\n",
            "Epoch 5, Training Loss= 0.736422, Training Accuracy= 0.82000, Validation Loss= 0.73972, Validation Accuracy= 0.87000\n",
            "Epoch 6, Training Loss= 0.728480, Training Accuracy= 0.83000, Validation Loss= 0.71244, Validation Accuracy= 0.82000\n",
            "Epoch 7, Training Loss= 0.694576, Training Accuracy= 0.78000, Validation Loss= 0.70887, Validation Accuracy= 0.80000\n",
            "Epoch 8, Training Loss= 0.653530, Training Accuracy= 0.85000, Validation Loss= 0.65857, Validation Accuracy= 0.81000\n",
            "Epoch 9, Training Loss= 0.607110, Training Accuracy= 0.82000, Validation Loss= 0.62715, Validation Accuracy= 0.90000\n",
            "Epoch 10, Training Loss= 0.573305, Training Accuracy= 0.85000, Validation Loss= 0.57264, Validation Accuracy= 0.79000\n",
            "Epoch 11, Training Loss= 0.557699, Training Accuracy= 0.80000, Validation Loss= 0.55436, Validation Accuracy= 0.83000\n",
            "Epoch 12, Training Loss= 0.537327, Training Accuracy= 0.84000, Validation Loss= 0.52197, Validation Accuracy= 0.85000\n",
            "Epoch 13, Training Loss= 0.505101, Training Accuracy= 0.85000, Validation Loss= 0.51077, Validation Accuracy= 0.90000\n",
            "Epoch 14, Training Loss= 0.466449, Training Accuracy= 0.89000, Validation Loss= 0.47203, Validation Accuracy= 0.84000\n",
            "Epoch 15, Training Loss= 0.482909, Training Accuracy= 0.89000, Validation Loss= 0.47617, Validation Accuracy= 0.86000\n",
            "Epoch 16, Training Loss= 0.449203, Training Accuracy= 0.90000, Validation Loss= 0.45631, Validation Accuracy= 0.90000\n",
            "Epoch 17, Training Loss= 0.429191, Training Accuracy= 0.86000, Validation Loss= 0.44462, Validation Accuracy= 0.82000\n",
            "Epoch 18, Training Loss= 0.404986, Training Accuracy= 0.90000, Validation Loss= 0.42831, Validation Accuracy= 0.86000\n",
            "Epoch 19, Training Loss= 0.365312, Training Accuracy= 0.90000, Validation Loss= 0.38029, Validation Accuracy= 0.91000\n",
            "Epoch 20, Training Loss= 0.329093, Training Accuracy= 0.91000, Validation Loss= 0.35978, Validation Accuracy= 0.92000\n",
            "Epoch 21, Training Loss= 0.309490, Training Accuracy= 0.89000, Validation Loss= 0.34791, Validation Accuracy= 0.92000\n",
            "Epoch 22, Training Loss= 0.294356, Training Accuracy= 0.86000, Validation Loss= 0.31433, Validation Accuracy= 0.84000\n",
            "Epoch 23, Training Loss= 0.266029, Training Accuracy= 0.92000, Validation Loss= 0.29221, Validation Accuracy= 0.91000\n",
            "Epoch 24, Training Loss= 0.270309, Training Accuracy= 0.96000, Validation Loss= 0.28005, Validation Accuracy= 0.89000\n",
            "Epoch 25, Training Loss= 0.258543, Training Accuracy= 0.94000, Validation Loss= 0.29011, Validation Accuracy= 0.90000\n",
            "Epoch 26, Training Loss= 0.235934, Training Accuracy= 0.94000, Validation Loss= 0.26257, Validation Accuracy= 0.93000\n",
            "Epoch 27, Training Loss= 0.222353, Training Accuracy= 0.96000, Validation Loss= 0.25709, Validation Accuracy= 0.93000\n",
            "Epoch 28, Training Loss= 0.394245, Training Accuracy= 0.89000, Validation Loss= 0.39512, Validation Accuracy= 0.87000\n",
            "Epoch 29, Training Loss= 0.264926, Training Accuracy= 0.94000, Validation Loss= 0.27069, Validation Accuracy= 0.93000\n",
            "Epoch 30, Training Loss= 0.235226, Training Accuracy= 0.86000, Validation Loss= 0.24518, Validation Accuracy= 0.94000\n",
            "Epoch 31, Training Loss= 0.221027, Training Accuracy= 0.94000, Validation Loss= 0.23835, Validation Accuracy= 0.95000\n",
            "Epoch 32, Training Loss= 0.201575, Training Accuracy= 0.93000, Validation Loss= 0.24298, Validation Accuracy= 0.89000\n",
            "Epoch 33, Training Loss= 0.172014, Training Accuracy= 0.97000, Validation Loss= 0.22474, Validation Accuracy= 0.95000\n",
            "Epoch 34, Training Loss= 0.221342, Training Accuracy= 0.95000, Validation Loss= 0.25981, Validation Accuracy= 0.93000\n",
            "Epoch 35, Training Loss= 0.190223, Training Accuracy= 0.95000, Validation Loss= 0.24356, Validation Accuracy= 0.93000\n",
            "Epoch 36, Training Loss= 0.179563, Training Accuracy= 0.98000, Validation Loss= 0.23389, Validation Accuracy= 0.92000\n",
            "Epoch 37, Training Loss= 0.165095, Training Accuracy= 0.95000, Validation Loss= 0.21659, Validation Accuracy= 0.95000\n",
            "Epoch 38, Training Loss= 0.167460, Training Accuracy= 0.92000, Validation Loss= 0.21583, Validation Accuracy= 0.97000\n",
            "Epoch 39, Training Loss= 0.182851, Training Accuracy= 0.93000, Validation Loss= 0.23763, Validation Accuracy= 0.94000\n",
            "Epoch 40, Training Loss= 0.179365, Training Accuracy= 0.99000, Validation Loss= 0.24833, Validation Accuracy= 0.91000\n",
            "Epoch 41, Training Loss= 0.157582, Training Accuracy= 0.96000, Validation Loss= 0.23407, Validation Accuracy= 0.94000\n",
            "Epoch 42, Training Loss= 0.157518, Training Accuracy= 0.98000, Validation Loss= 0.21038, Validation Accuracy= 0.98000\n",
            "Epoch 43, Training Loss= 0.153931, Training Accuracy= 0.96000, Validation Loss= 0.21656, Validation Accuracy= 0.91000\n",
            "Epoch 44, Training Loss= 0.139514, Training Accuracy= 0.95000, Validation Loss= 0.20984, Validation Accuracy= 0.91000\n",
            "Epoch 45, Training Loss= 0.141827, Training Accuracy= 0.95000, Validation Loss= 0.19278, Validation Accuracy= 0.92000\n",
            "Epoch 46, Training Loss= 0.130246, Training Accuracy= 0.91000, Validation Loss= 0.21291, Validation Accuracy= 0.93000\n",
            "Epoch 47, Training Loss= 0.211245, Training Accuracy= 0.94000, Validation Loss= 0.25546, Validation Accuracy= 0.93000\n",
            "Epoch 48, Training Loss= 0.144744, Training Accuracy= 0.97000, Validation Loss= 0.22188, Validation Accuracy= 0.90000\n",
            "Epoch 49, Training Loss= 0.141738, Training Accuracy= 0.96000, Validation Loss= 0.20246, Validation Accuracy= 0.93000\n",
            "Epoch 50, Training Loss= 0.129042, Training Accuracy= 0.95000, Validation Loss= 0.19862, Validation Accuracy= 0.96000\n",
            "Epoch 51, Training Loss= 0.136736, Training Accuracy= 0.96000, Validation Loss= 0.19475, Validation Accuracy= 0.94000\n",
            "Epoch 52, Training Loss= 0.119105, Training Accuracy= 1.00000, Validation Loss= 0.18945, Validation Accuracy= 0.92000\n",
            "Epoch 53, Training Loss= 0.114742, Training Accuracy= 0.98000, Validation Loss= 0.19112, Validation Accuracy= 0.94000\n",
            "Epoch 54, Training Loss= 0.133812, Training Accuracy= 0.99000, Validation Loss= 0.20441, Validation Accuracy= 0.97000\n",
            "Epoch 55, Training Loss= 0.126358, Training Accuracy= 0.94000, Validation Loss= 0.20476, Validation Accuracy= 0.94000\n",
            "Epoch 56, Training Loss= 0.116328, Training Accuracy= 0.98000, Validation Loss= 0.18913, Validation Accuracy= 0.92000\n",
            "Epoch 57, Training Loss= 0.116187, Training Accuracy= 0.97000, Validation Loss= 0.21577, Validation Accuracy= 0.98000\n",
            "Epoch 58, Training Loss= 0.105176, Training Accuracy= 0.96000, Validation Loss= 0.18803, Validation Accuracy= 0.95000\n",
            "Epoch 59, Training Loss= 0.108786, Training Accuracy= 0.95000, Validation Loss= 0.18256, Validation Accuracy= 0.99000\n",
            "Epoch 60, Training Loss= 0.115859, Training Accuracy= 0.97000, Validation Loss= 0.18009, Validation Accuracy= 0.91000\n",
            "Epoch 61, Training Loss= 0.100005, Training Accuracy= 0.98000, Validation Loss= 0.17069, Validation Accuracy= 0.97000\n",
            "Epoch 62, Training Loss= 0.105810, Training Accuracy= 0.97000, Validation Loss= 0.19876, Validation Accuracy= 0.95000\n",
            "Epoch 63, Training Loss= 0.098409, Training Accuracy= 0.97000, Validation Loss= 0.18706, Validation Accuracy= 0.96000\n",
            "Epoch 64, Training Loss= 0.101404, Training Accuracy= 0.98000, Validation Loss= 0.17804, Validation Accuracy= 0.94000\n",
            "Epoch 65, Training Loss= 0.100529, Training Accuracy= 0.97000, Validation Loss= 0.17832, Validation Accuracy= 0.97000\n",
            "Epoch 66, Training Loss= 0.087485, Training Accuracy= 0.97000, Validation Loss= 0.17826, Validation Accuracy= 0.90000\n",
            "Epoch 67, Training Loss= 0.110141, Training Accuracy= 0.98000, Validation Loss= 0.19726, Validation Accuracy= 0.96000\n",
            "Epoch 68, Training Loss= 0.097869, Training Accuracy= 0.98000, Validation Loss= 0.18100, Validation Accuracy= 0.97000\n",
            "Epoch 69, Training Loss= 0.093836, Training Accuracy= 0.96000, Validation Loss= 0.18504, Validation Accuracy= 1.00000\n",
            "Epoch 70, Training Loss= 0.086752, Training Accuracy= 0.96000, Validation Loss= 0.16049, Validation Accuracy= 0.94000\n",
            "Epoch 71, Training Loss= 0.096173, Training Accuracy= 0.98000, Validation Loss= 0.18712, Validation Accuracy= 0.98000\n",
            "Epoch 72, Training Loss= 0.094153, Training Accuracy= 0.96000, Validation Loss= 0.18738, Validation Accuracy= 0.95000\n",
            "Epoch 73, Training Loss= 0.074006, Training Accuracy= 0.98000, Validation Loss= 0.18514, Validation Accuracy= 0.95000\n",
            "Epoch 74, Training Loss= 0.078643, Training Accuracy= 0.96000, Validation Loss= 0.18884, Validation Accuracy= 0.95000\n",
            "Epoch 75, Training Loss= 0.076857, Training Accuracy= 1.00000, Validation Loss= 0.18529, Validation Accuracy= 0.98000\n",
            "Epoch 76, Training Loss= 0.070925, Training Accuracy= 0.99000, Validation Loss= 0.17004, Validation Accuracy= 0.96000\n",
            "Epoch 77, Training Loss= 0.082720, Training Accuracy= 0.98000, Validation Loss= 0.18448, Validation Accuracy= 0.93000\n",
            "Epoch 78, Training Loss= 0.080235, Training Accuracy= 0.97000, Validation Loss= 0.17446, Validation Accuracy= 0.92000\n",
            "Epoch 79, Training Loss= 0.074835, Training Accuracy= 0.99000, Validation Loss= 0.18219, Validation Accuracy= 0.95000\n",
            "Epoch 80, Training Loss= 0.073113, Training Accuracy= 0.99000, Validation Loss= 0.17541, Validation Accuracy= 0.96000\n",
            "Epoch 81, Training Loss= 0.072754, Training Accuracy= 0.98000, Validation Loss= 0.19430, Validation Accuracy= 0.96000\n",
            "Epoch 82, Training Loss= 0.079049, Training Accuracy= 1.00000, Validation Loss= 0.19330, Validation Accuracy= 0.94000\n",
            "Epoch 83, Training Loss= 0.072589, Training Accuracy= 0.97000, Validation Loss= 0.18958, Validation Accuracy= 0.95000\n",
            "Epoch 84, Training Loss= 0.070242, Training Accuracy= 0.98000, Validation Loss= 0.17379, Validation Accuracy= 0.94000\n",
            "Epoch 85, Training Loss= 0.074168, Training Accuracy= 0.99000, Validation Loss= 0.17958, Validation Accuracy= 0.99000\n",
            "Epoch 86, Training Loss= 0.073139, Training Accuracy= 0.97000, Validation Loss= 0.18763, Validation Accuracy= 0.96000\n",
            "Epoch 87, Training Loss= 0.067777, Training Accuracy= 0.97000, Validation Loss= 0.16489, Validation Accuracy= 0.96000\n",
            "Epoch 88, Training Loss= 0.066349, Training Accuracy= 0.97000, Validation Loss= 0.19497, Validation Accuracy= 0.92000\n",
            "Epoch 89, Training Loss= 0.068712, Training Accuracy= 0.96000, Validation Loss= 0.18446, Validation Accuracy= 0.93000\n",
            "Epoch 90, Training Loss= 0.058833, Training Accuracy= 1.00000, Validation Loss= 0.17619, Validation Accuracy= 0.96000\n",
            "Epoch 91, Training Loss= 0.060680, Training Accuracy= 0.98000, Validation Loss= 0.16057, Validation Accuracy= 0.92000\n",
            "Epoch 92, Training Loss= 0.060648, Training Accuracy= 0.99000, Validation Loss= 0.19101, Validation Accuracy= 0.98000\n",
            "Epoch 93, Training Loss= 0.064749, Training Accuracy= 0.98000, Validation Loss= 0.19166, Validation Accuracy= 0.99000\n",
            "Epoch 94, Training Loss= 0.062702, Training Accuracy= 0.96000, Validation Loss= 0.18458, Validation Accuracy= 0.93000\n",
            "Epoch 95, Training Loss= 0.057615, Training Accuracy= 0.98000, Validation Loss= 0.16849, Validation Accuracy= 0.94000\n",
            "Epoch 96, Training Loss= 0.058443, Training Accuracy= 0.98000, Validation Loss= 0.17665, Validation Accuracy= 0.97000\n",
            "Epoch 97, Training Loss= 0.060881, Training Accuracy= 0.98000, Validation Loss= 0.17067, Validation Accuracy= 0.94000\n",
            "Epoch 98, Training Loss= 0.063200, Training Accuracy= 0.96000, Validation Loss= 0.17725, Validation Accuracy= 0.98000\n",
            "Epoch 99, Training Loss= 0.050292, Training Accuracy= 0.97000, Validation Loss= 0.17070, Validation Accuracy= 0.97000\n",
            "Optimization Finished!\n",
            "('Validation Accuracy:', 0.96046454)\n",
            "('Testing Accuracy:', 0.96077067)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbcDLS3a-EiL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Single prediction\n",
        "#importing file\n",
        "train_audio_path = '/content/gdrive/My Drive/Colab Notebooks/'\n",
        "samples, sample_rate = librosa.load(train_audio_path+'fffcabd1_nohash_0.wav', sr = 16000)\n",
        "#geting mfccs\n",
        "mfcc1 = mfcc(samples, samplerate=sample_rate, numcep=13, nfilt=26, nfft = 512)\n",
        "mfcc1 = mfcc1[np.newaxis, :, :]\n",
        "#normalization of data\n",
        "num_instances1, num_time_steps1, num_features1 = mfcc1.shape\n",
        "mfcc1 = np.reshape(mfcc1, newshape=(-1, num_features1))\n",
        "mfcc1 = sc.fit_transform(mfcc1)\n",
        "mfcc1 = np.reshape(mfcc1, newshape=(num_instances1, num_time_steps1, num_features1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vISHplRI7jWj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "#Single prediction\n",
        "#importing file\n",
        "train_audio_path = '/content/gdrive/My Drive/Colab Notebooks/'\n",
        "samples, sample_rate = librosa.load(train_audio_path+'0c40e715_nohash_0.wav', sr = 16000)\n",
        "#geting mfccs\n",
        "mfcc2 = mfcc(samples, samplerate=sample_rate, numcep=13, nfilt=26, nfft = 512)\n",
        "mfcc2 = mfcc2[np.newaxis, :, :]\n",
        "#normalization of data\n",
        "num_instances2, num_time_steps2, num_features2 = mfcc2.shape\n",
        "mfcc2 = np.reshape(mfcc2, newshape=(-1, num_features2))\n",
        "mfcc2 = sc.fit_transform(mfcc2)\n",
        "mfcc2 = np.reshape(mfcc2, newshape=(num_instances2, num_time_steps2, num_features2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8HUcTNaT7Jy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#my_voice example 'bed'\n",
        "train_audio_path = '/content/gdrive/My Drive/Colab Notebooks/my_voice/'\n",
        "samples, sample_rate = librosa.load(train_audio_path+'dog3.wav', sr = 16000)\n",
        "#geting mfccs\n",
        "mfcc_bed = mfcc(samples, samplerate=sample_rate, numcep=13, nfilt=26, nfft = 512)\n",
        "mfcc_bed = mfcc_bed[np.newaxis, :, :]\n",
        "#normalization of data\n",
        "num_instances_bed, num_time_steps_bed, num_features_bed = mfcc_bed.shape\n",
        "mfcc_bed = np.reshape(mfcc_bed, newshape=(-1, num_features_bed))\n",
        "mfcc_bed = sc.fit_transform(mfcc_bed)\n",
        "mfcc_bed = np.reshape(mfcc_bed, newshape=(num_instances_bed, num_time_steps_bed, num_features_bed))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3YTTU22U9zG",
        "colab_type": "code",
        "outputId": "f2f122dd-00cb-49c0-af18-3d1497481617",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "with tf.Session() as sess:\n",
        "  # Restore variables from disk.\n",
        "  sess.run(init)\n",
        "  saver.restore(sess, \"my_test_model\")\n",
        "  print(\"Model restored.\")\n",
        "  y_example_bed = tf.nn.softmax(pred)\n",
        "  y_pred_bed = sess.run(y_example_bed, feed_dict={x: mfcc_bed})\n",
        "  print(y_pred_bed)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model restored.\n",
            "[[4.344317e-06 2.868859e-08 9.253901e-01 5.992488e-06 5.713409e-04\n",
            "  7.402736e-02 7.359995e-07]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sL5jEn8QVSL8",
        "colab_type": "code",
        "outputId": "9e370c18-79f3-4730-b19f-2ad4209f1274",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "result_bed = np.where(y_pred_bed == y_pred_bed.max())\n",
        "print(result_bed)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(array([0]), array([2]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uM937PkVZ_W",
        "colab_type": "code",
        "outputId": "bee9c96a-c2c3-4542-b56b-e41b58cf283a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "classes"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bed', 'cat', 'dog', 'one', 'stop', 'unknown', 'zero']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vr2EFqarT-Wr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#my_voice example 'john'\n",
        "train_audio_path = '/content/gdrive/My Drive/Colab Notebooks/my_voice/'\n",
        "samples, sample_rate = librosa.load(train_audio_path+'stop1.wav', sr = 16000)\n",
        "#geting mfccs\n",
        "mfcc_john = mfcc(samples, samplerate=sample_rate, numcep=13, nfilt=26, nfft = 512)\n",
        "mfcc_john = mfcc_john[np.newaxis, :, :]\n",
        "#normalization of data\n",
        "num_instances_john, num_time_steps_john, num_features_john = mfcc_john.shape\n",
        "mfcc_john = np.reshape(mfcc_john, newshape=(-1, num_features_john))\n",
        "mfcc_john = sc.fit_transform(mfcc_john)\n",
        "mfcc_john = np.reshape(mfcc_john, newshape=(num_instances_john, num_time_steps_john, num_features_john))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YI_hObAFVf5m",
        "colab_type": "code",
        "outputId": "03c4f667-42fc-44e0-f337-2f95fb04726d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "with tf.Session() as sess:\n",
        "  # Restore variables from disk.\n",
        "  sess.run(init)\n",
        "  saver.restore(sess, \"my_test_model\")\n",
        "  print(\"Model restored.\")\n",
        "  y_example_john = tf.nn.softmax(pred)\n",
        "  y_pred_john = sess.run(y_example_john, feed_dict={x: mfcc_john})\n",
        "  print(y_pred_john)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model restored.\n",
            "[[9.0181529e-10 2.1667731e-06 8.9769623e-05 1.0510327e-06 9.9987519e-01\n",
            "  3.0381114e-05 1.4273786e-06]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQ1HXFwlVfuI",
        "colab_type": "code",
        "outputId": "173650d5-ce70-4382-a520-e432ee3bc499",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "result_john = np.where(y_pred_john == y_pred_john.max())\n",
        "print(result_john)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(array([0]), array([4]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hxqvq-dpVfj8",
        "colab_type": "code",
        "outputId": "71254925-aa9e-4624-fce8-711b01c3cb35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "classes"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bed', 'cat', 'dog', 'one', 'stop', 'unknown', 'zero']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O940L6TUIwB6",
        "colab_type": "code",
        "outputId": "ceb924c8-b420-43d2-8642-1d13a6ffbad9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "with tf.Session() as sess:\n",
        "  # Restore variables from disk.\n",
        "  sess.run(init)\n",
        "  saver.restore(sess, \"my_test_model\")\n",
        "  print(\"Model restored.\")\n",
        "  y_example = tf.nn.softmax(pred)\n",
        "  y_pred = sess.run(y_example, feed_dict={x: mfcc1})\n",
        "  print(y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model restored.\n",
            "[[1.2210573e-06 5.5986402e-07 1.1723056e-06 1.7017009e-06 6.7940624e-07\n",
            "  9.2707167e-04 9.9906760e-01]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWMMAQes9Ocg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.Session() as sess:\n",
        "  # Restore variables from disk.\n",
        "  saver.restore(sess, \"content/my_test_model\")\n",
        "  print(\"Model restored.\")\n",
        "  y_example2 = tf.nn.softmax(pred)\n",
        "  y_pred2 = sess.run(y_example2, feed_dict={x: mfcc2})\n",
        "  print(y_pred2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9zCSwki-NHr",
        "colab_type": "code",
        "outputId": "d066ac9d-6f5a-4e8a-9c4b-811cdae906d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "result2 = np.where(y_pred2 == y_pred2.max())\n",
        "print(result2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(array([0]), array([0]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KsjQ1fv9VjV",
        "colab_type": "code",
        "outputId": "7283249e-8262-4fa0-eca3-289ccd38198f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "result = np.where(y_pred == y_pred.max())\n",
        "print(result)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(array([0]), array([6]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kDOjKf9FkOq",
        "colab_type": "code",
        "outputId": "ed67ef2f-1cfb-452f-b6b3-6c5664cf35f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "classes"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bed', 'cat', 'dog', 'one', 'stop', 'unknown', 'zero']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    }
  ]
}